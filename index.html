<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="preload" as="image" href="/icons/xiaohongshu.png"/><link rel="stylesheet" href="/_next/static/css/19bf7477d82f620e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-dc513d0168f29800.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-8723656a45eb45d5.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-485beed672c66dd8.js" async=""></script><script src="/_next/static/chunks/app/page-956705dee824468e.js" async=""></script><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Yixuan Li</title><meta name="description" content="Ph.D. student at the Beijing Institute of Technology."/><meta name="author" content="Yixuan Li"/><meta name="keywords" content="Yixuan Li,PhD,Research,Perception, Interaction, Embodiment Lab, @ Beijing Institute of Technology &amp; Beijing Institute for General Artificial Intelligence"/><meta name="creator" content="Yixuan Li"/><meta name="publisher" content="Yixuan Li"/><meta property="og:title" content="Yixuan Li"/><meta property="og:description" content="Ph.D. student at the Beijing Institute of Technology."/><meta property="og:site_name" content="Yixuan Li&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Yixuan Li"/><meta name="twitter:description" content="Ph.D. student at the Beijing Institute of Technology."/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Yixuan Li</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-¬´R5pdb¬ª" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Yixuan Li" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Yixuan Li</h1><p class="text-lg text-accent font-medium mb-1">Ph.D. Student</p><p class="text-neutral-600 mb-2">Perception, Interaction, Embodiment Lab, @ Beijing Institute of Technology &amp; Beijing Institute for General Artificial Intelligence</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><a href="https://scholar.google.com/citations?user=xZm0IygAAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/0009-0001-0650-4133" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/yixxuan-li" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://www.linkedin.com/in/yixuan-li-638b81157/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-5 w-5" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><a href="https://www.xiaohongshu.com/user/profile/623181130000000002022a83" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Xiaohongshu"><img src="/icons/xiaohongshu.png" alt="Xiaohongshu Icon" class="h-5 w-5" style="width:24px;height:24px;object-fit:contain"/></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>Robotics Learning</div><div>Humanoids</div></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a third-year Ph.D. student at the School of Computer Science and Technology, <a href="https://bit.edu.cn" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Beijing Institute of Technology (BIT)</a>. I am a member of <a href="http://pie-lab.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Perception, Interaction, Embodiment Lab (Pie Lab), BIT</a> under the supervision of Prof. <a href="https://cs.bit.edu.cn/szdw/jsml2/txjsygzznyjs2/8b9943e7a8984a65a3ba3c3e6b3b161b.htm" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Wei Liang</a>. I currently serve as a research intern at the <a href="https://www.bigai.ai/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Beijing Institute for General Artificial Intelligence (BIGAI)</a>, advised by Dr. <a href="https://siyuanhuang.com" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Siyuan Huang</a>, Director of the Center of Embodied AI and Robotics.  I earned my Bachelor degree(Intelligence Science and Technology) in 2021 from <a href="https://www.qdu.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Qingdao University</a>.</p></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/#publications">View All ‚Üí</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Learning Human-Humanoid Coordination for Collaborative Object Carrying</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Yushi Du</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">*</sup>, </span><span><span class="font-semibold text-accent">Yixuan Li</span><sup class="ml-0 text-accent">*</sup>, </span><span><span class="">Baoxiong Jia</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup><sup class="ml-0 text-neutral-600 dark:text-neutral-500">*</sup>, </span><span><span class="">Yutang Lin</span>, </span><span><span class="">Pei Zhou</span>, </span><span><span class="">Wei Liang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Yanchao Yang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Siyuan Huang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">arXiv preprint arXiv:2510.14293</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Yixuan Li</span><sup class="ml-0 text-accent">*</sup>, </span><span><span class="">Yutang Lin</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">*</sup>, </span><span><span class="">Jieming Cui</span>, </span><span><span class="">Tengyu Liu</span>, </span><span><span class="">Wei Liang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Yixin Zhu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Siyuan Huang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Proceedings of The 9th Conference on Robot Learning (CoRL)</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Env-Mani: Quadrupedal Robot Loco-Manipulation with Environment-in-the-Loop</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Yixuan Li</span>, </span><span><span class="">Zan Wang</span>, </span><span><span class="">Wei Liang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">R2G: Reasoning to ground in 3D scenes</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Yixuan Li</span>, </span><span><span class="">Zan Wang</span>, </span><span><span class="">Wei Liang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Pattern Recognition</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->Dec 1, 2025</p><p class="text-xs text-neutral-500">Photo taken by<span class="ml-2"></span><a href="https://ariesssxu.github.io/" target="_blank" rel="noopener noreferrer">Manjie Xu</a></p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-8723656a45eb45d5.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-8723656a45eb45d5.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-8723656a45eb45d5.js\"],\"default\"]\n7:I[7437,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-485beed672c66dd8.js\",\"974\",\"static/chunks/app/page-956705dee824468e.js\"],\"default\"]\n8:I[9507,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-485beed672c66dd8.js\",\"974\",\"static/chunks/app/page-956705dee824468e.js\"],\"default\"]\n9:I[5218,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-485beed672c66dd8.js\",\"974\",\"static/chunks/app/page-956705dee824468e.js\"],\"default\"]\n12:I[9665,[],\"MetadataBoundary\"]\n14:I[9665,[],\"OutletBoundary\"]\n17:I[4911,[],\"AsyncMetadataOutlet\"]\n19:I[9665,[],\"ViewportBoundary\"]\n1b:I[6614,[],\"\"]\n:HL[\"/_next/static/css/19bf7477d82f620e.css\",\"style\"]\na:T64b,Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioceptio"])</script><script>self.__next_f.push([1,"n-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.b:T811,"])</script><script>self.__next_f.push([1,"@inproceedings{du2025learning,\n  title = {Learning Human-Humanoid Coordination for Collaborative Object Carrying},\n  author = {Du, Yushi and Li, Yixuan and Jia, Baoxiong and Lin, Yutang and Zhou, Pei and Liang, Wei and Yang, Yanchao and Huang, Siyuan},\n  journal = {arXiv preprint arXiv:2510.14293},\n  year = {2025},\n  pages = {https://yushi-du.github.io/COLA/},\n  pdfUrl = {https://arxiv.org/pdf/2510.14293},\n  status = {under-review},\n  abstract = {Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.}\n}"])</script><script>self.__next_f.push([1,"c:T565,Humanoid robot teleoperation plays a vital role in demonstrating and collecting data for complex interactions. Current methods suffer from two key limitations: (1) restricted controllability due to decoupled upper- and lower-body control, and (2) severe drift caused by open-loop execution. These issues prevent humanoid robots from performing coordinated whole-body motions required for long-horizon loco-manipulation tasks. We introduce CLONE, a whole-body teleoperation system that overcomes these challenges through three key contributions: (1) a Mixture-of-Experts (MoE) whole-body control policy that enables complex coordinated movements, such as ‚Äúpicking up an object from the ground‚Äù and ‚Äúplacing it in a distant bin‚Äù; (2) a closed-loop error correction mechanism using LiDAR odometry, reducing translational drift to 12cm over 8.9-meter trajectories; and (3) a systematic data augmentation strategy that ensures robust performance under diverse, previously unseen operator poses. In extensive experiments, CLONE demonstrates robust performance across diverse scenarios while maintaining stable whole-body control. These capabilities significantly advance humanoid robotics by enabling the collection of long-horizon interaction data and establishing a foundation for more sophisticated humanoid-environment interaction in both research and practical applications.d:T720,@inproceedings{li2025clone,\n  title = {CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks},\n  author = {Li, Yixuan and Lin, Yutang and Cui, Jieming and Liu, Tengyu and Liang, Wei and Zhu, Yixin and Huang, Siyuan},\n  booktitle = {Proceedings of The 9th Conference on Robot Learning (CoRL)},\n  year = {2025},\n  pages = {https://humanoid-clone.github.io/},\n  pdfUrl = {https://arxiv.org/pdf/2506.08931},\n  abstract = {Humanoid robot teleoperation plays a vital role in demonstrating and collecting data for complex interactions. Current methods suffer from two key limitations: (1) restricted controllability due to decoupled upper"])</script><script>self.__next_f.push([1,"- and lower-body control, and (2) severe drift caused by open-loop execution. These issues prevent humanoid robots from performing coordinated whole-body motions required for long-horizon loco-manipulation tasks. We introduce CLONE, a whole-body teleoperation system that overcomes these challenges through three key contributions: (1) a Mixture-of-Experts (MoE) whole-body control policy that enables complex coordinated movements, such as ‚Äúpicking up an object from the ground‚Äù and ‚Äúplacing it in a distant bin‚Äù; (2) a closed-loop error correction mechanism using LiDAR odometry, reducing translational drift to 12cm over 8.9-meter trajectories; and (3) a systematic data augmentation strategy that ensures robust performance under diverse, previously unseen operator poses. In extensive experiments, CLONE demonstrates robust performance across diverse scenarios while maintaining stable whole-body control. These capabilities significantly advance humanoid robotics by enabling the collection of long-horizon interaction data and establishing a foundation for more sophisticated humanoid-environment interaction in both research and practical applications.}\n}e:T571,Dogs can climb onto tables using their front legs for support, enabling them to retrieve objects and significantly expand their workspace by leveraging the external environment. However, the ability of quadrupedal robots to perform similar skills remains largely unexplored. In this work, we introduce a unified, learning-based loco-manipulation framework for quadrupedal robots, allowing them to utilize the external environment as support to extend their workspace and enhance their manipulation capabilities. Specifically, our method proposes a unified policy that takes limited onboard sensors and proprioception as input, generating whole-body actions that enable the robot to manipulate objects. To guide the policy learning for environment-in-the-loop manipulation, we design a set of rewards that address challenges such as imprecise perception and center-of-mas"])</script><script>self.__next_f.push([1,"s shifts. Additionally, we employ curriculum learning to train both teacher and student policies, ensuring effective skill transfer in complex tasks. We train the policy in simulation and conduct extensive experiments, demonstrating that our approach allows robots to manipulate previously inaccessible objects, opening up new possibilities for enhancing quadrupedal robot capabilities without the need for hardware modifications or additional costs. The project page is available at https://sites.google.com/view/env-mani.f:T737,@inproceedings{li2025env-mani,\n  author = {Li, Yixuan and Wang, Zan and Liang, Wei},\n  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n  title = {Env-Mani: Quadrupedal Robot Loco-Manipulation with Environment-in-the-Loop},\n  year = {2025},\n  doi = {10.1109/IROS60139.2025.11246108},\n  pages = {https://sites.google.com/view/env-mani},\n  pdfUrl = {https://ieeexplore.ieee.org/document/11246108},\n  abstract = {Dogs can climb onto tables using their front legs for support, enabling them to retrieve objects and significantly expand their workspace by leveraging the external environment. However, the ability of quadrupedal robots to perform similar skills remains largely unexplored. In this work, we introduce a unified, learning-based loco-manipulation framework for quadrupedal robots, allowing them to utilize the external environment as support to extend their workspace and enhance their manipulation capabilities. Specifically, our method proposes a unified policy that takes limited onboard sensors and proprioception as input, generating whole-body actions that enable the robot to manipulate objects. To guide the policy learning for environment-in-the-loop manipulation, we design a set of rewards that address challenges such as imprecise perception and center-of-mass shifts. Additionally, we employ curriculum learning to train both teacher and student policies, ensuring effective skill transfer in complex tasks. We train the policy in simulation and conduct e"])</script><script>self.__next_f.push([1,"xtensive experiments, demonstrating that our approach allows robots to manipulate previously inaccessible objects, opening up new possibilities for enhancing quadrupedal robot capabilities without the need for hardware modifications or additional costs. The project page is available at https://sites.google.com/view/env-mani.}\n}10:T5a9,We propose Reasoning to Ground (R2G), a neural symbolic model that grounds the target objects in 3D scenes in a reasoning manner. Unlike previous works that rely on end-to-end models for grounding, which often function as black boxes, our approach seeks to provide a more interpretable and reliable solution. R2Gexplicitly models the 3D scene using a semantic concept-based scene graph, recurrently simulates the attention transferring across object entities, and interpretably grounding the target objects with the highest attention score. Specifically, we embed multiple object properties within the graph nodes and spatial relations among entities within the edges through a predefined semantic vocabulary. To guide attention transfer, we employ learning or prompting-based approaches to interpret the referential utterance into reasoning instructions within the same semantic space. In each reasoning round, we either (1) merge current attention distribution with the similarity between instructions and embedded entity properties, or (2) shift the attention across the scene graph based on the similarity between instructions and embedded spatial relations. The experiments on Sr3D/Nr3D benchmarks show that R2G achieves a comparable result with the prior works while offering improved interpretability, breaking a new path for 3D grounding. The code and dataset for this work are available at:https://sites.google.com/view/reasoning-to-ground.11:T720,@article{li2025r2g,\n  title = {R2G: Reasoning to ground in 3D scenes},\n  journal = {Pattern Recognition},\n  volume = {168},\n  pages = {https://sites.google.com/view/reasoning-to-ground},\n  year = {2025},\n  doi = {https://doi.org/10.1016/j.patcog.2025.111"])</script><script>self.__next_f.push([1,"728},\n  author = {Yixuan Li and Zan Wang and Wei Liang},\n  pdfUrl = {https://arxiv.org/pdf/2408.13499},\n  abstract = {We propose Reasoning to Ground (R2G), a neural symbolic model that grounds the target objects in 3D scenes in a reasoning manner. Unlike previous works that rely on end-to-end models for grounding, which often function as black boxes, our approach seeks to provide a more interpretable and reliable solution. R2Gexplicitly models the 3D scene using a semantic concept-based scene graph, recurrently simulates the attention transferring across object entities, and interpretably grounding the target objects with the highest attention score. Specifically, we embed multiple object properties within the graph nodes and spatial relations among entities within the edges through a predefined semantic vocabulary. To guide attention transfer, we employ learning or prompting-based approaches to interpret the referential utterance into reasoning instructions within the same semantic space. In each reasoning round, we either (1) merge current attention distribution with the similarity between instructions and embedded entity properties, or (2) shift the attention across the scene graph based on the similarity between instructions and embedded spatial relations. The experiments on Sr3D/Nr3D benchmarks show that R2G achieves a comparable result with the prior works while offering improved interpretability, breaking a new path for 3D grounding. The code and dataset for this work are available at:https://sites.google.com/view/reasoning-to-ground.}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"FWm2lGG-93l0xAvu86y8w\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/19bf7477d82f620e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"}],\"siteTitle\":\"Yixuan Li\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"Dec 1, 2025\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Yixuan Li\",\"title\":\"Ph.D. Student\",\"institution\":\"Perception, Interaction, Embodiment Lab, @ Beijing Institute of Technology \u0026 Beijing Institute for General Artificial Intelligence\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"liyixxuan@gmail.com\",\"google_scholar\":\"https://scholar.google.com/citations?user=xZm0IygAAAAJ\u0026hl=zh-CN\",\"orcid\":\"https://orcid.org/0009-0001-0650-4133\",\"github\":\"https://github.com/yixxuan-li\",\"linkedin\":\"https://www.linkedin.com/in/yixuan-li-638b81157/\",\"xiaohongshu\":\"https://www.xiaohongshu.com/user/profile/623181130000000002022a83\"},\"features\":{\"enable_likes\":false,\"enable_one_page_mode\":false},\"researchInterests\":[\"Robotics Learning\",\"Humanoids\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am a third-year Ph.D. student at the School of Computer Science and Technology, [Beijing Institute of Technology (BIT)](https://bit.edu.cn). I am a member of [Perception, Interaction, Embodiment Lab (Pie Lab), BIT](http://pie-lab.cn/) under the supervision of Prof. [Wei Liang](https://cs.bit.edu.cn/szdw/jsml2/txjsygzznyjs2/8b9943e7a8984a65a3ba3c3e6b3b161b.htm). I currently serve as a research intern at the [Beijing Institute for General Artificial Intelligence (BIGAI)](https://www.bigai.ai/), advised by Dr. [Siyuan Huang](https://siyuanhuang.com), Director of the Center of Embodied AI and Robotics.  I earned my Bachelor degree(Intelligence Science and Technology) in 2021 from [Qingdao University](https://www.qdu.edu.cn/).\",\"title\":\"About\"}],[\"$\",\"$L9\",\"featured_publications\",{\"publications\":[{\"id\":\"du2025learning\",\"title\":\"Learning Human-Humanoid Coordination for Collaborative Object Carrying\",\"authors\":[{\"name\":\"Yushi Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":true},{\"name\":\"Yixuan Li\",\"isHighlighted\":true,\"isCorresponding\":false,\"isMainAuthor\":true},{\"name\":\"Baoxiong Jia\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":true},{\"name\":\"Yutang Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Pei Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Wei Liang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false},{\"name\":\"Yanchao Yang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false},{\"name\":\"Siyuan Huang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2510.14293\",\"conference\":\"\",\"pages\":\"https://yushi-du.github.io/COLA/\",\"pdfUrl\":\"https://arxiv.org/pdf/2510.14293\",\"code\":\"https://github.com/Yushi-Du/COLA_Code\",\"abstract\":\"$a\",\"description\":\"COLA is a proprioception-only reinforcement learning approach that combines leader and follower behaviors within a single policy, enabling compliant human-humanoid collaborative carrying.\",\"selected\":true,\"preview\":\"cola.png\",\"bibtex\":\"$b\"},{\"id\":\"li2025clone\",\"title\":\"CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks\",\"authors\":[{\"name\":\"Yixuan Li\",\"isHighlighted\":true,\"isCorresponding\":false,\"isMainAuthor\":true},{\"name\":\"Yutang Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":true},{\"name\":\"Jieming Cui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Tengyu Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Wei Liang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false},{\"name\":\"Yixin Zhu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false},{\"name\":\"Siyuan Huang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of The 9th Conference on Robot Learning (CoRL)\",\"pages\":\"https://humanoid-clone.github.io/\",\"pdfUrl\":\"https://arxiv.org/pdf/2506.08931\",\"code\":\"https://github.com/humanoid-clone/CLONE/\",\"abstract\":\"$c\",\"description\":\"CLONE is a whole-body teleoperation system that overcomes the limitations of decoupled upper- and lower-body control and open-loop execution.\",\"selected\":true,\"preview\":\"clone.png\",\"bibtex\":\"$d\"},{\"id\":\"li2025env-mani\",\"title\":\"Env-Mani: Quadrupedal Robot Loco-Manipulation with Environment-in-the-Loop\",\"authors\":[{\"name\":\"Yixuan Li\",\"isHighlighted\":true,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Zan Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Wei Liang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"pages\":\"https://sites.google.com/view/env-mani\",\"pdfUrl\":\"https://ieeexplore.ieee.org/document/11246108\",\"doi\":\"10.1109/IROS60139.2025.11246108\",\"abstract\":\"$e\",\"description\":\"Env-Mani is a unified, learning-based loco-manipulation framework for quadrupedal robots that allows them to utilize the external environment as support to extend their workspace and enhance their manipulation capabilities.\",\"selected\":true,\"preview\":\"env_manip.png\",\"bibtex\":\"$f\"},{\"id\":\"li2025r2g\",\"title\":\"R2G: Reasoning to ground in 3D scenes\",\"authors\":[{\"name\":\"Yixuan Li\",\"isHighlighted\":true,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Zan Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isMainAuthor\":false},{\"name\":\"Wei Liang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isMainAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Pattern Recognition\",\"conference\":\"\",\"volume\":\"168\",\"pages\":\"https://sites.google.com/view/reasoning-to-ground\",\"pdfUrl\":\"https://arxiv.org/pdf/2408.13499\",\"doi\":\"https://doi.org/10.1016/j.patcog.2025.111728\",\"code\":\"https://github.com/yixxuan-li/R2G\",\"abstract\":\"$10\",\"description\":\"R2G is a neural symbolic model that grounds the target objects in 3D scenes in a reasoning manner.\",\"selected\":true,\"preview\":\"r2g.png\",\"bibtex\":\"$11\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":true}]],false,false,false]}]]}]]}]}],[\"$\",\"$L12\",null,{\"children\":\"$L13\"}],null,[\"$\",\"$L14\",null,{\"children\":[\"$L15\",\"$L16\",[\"$\",\"$L17\",null,{\"promise\":\"$@18\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"F2yZiSJm4SnyQ1a2Io2T7\",{\"children\":[[\"$\",\"$L19\",null,{\"children\":\"$L1a\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$1b\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1c:\"$Sreact.suspense\"\n1d:I[4911,[],\"AsyncMetadata\"]\n13:[\"$\",\"$1c\",null,{\"fallback\":null,\"children\":[\"$\",\"$L1d\",null,{\"promise\":\"$@1e\"}]}]\n"])</script><script>self.__next_f.push([1,"16:null\n"])</script><script>self.__next_f.push([1,"1a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n15:null\n"])</script><script>self.__next_f.push([1,"1e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Yixuan Li\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Ph.D. student at the Beijing Institute of Technology.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Yixuan Li\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Yixuan Li,PhD,Research,Perception, Interaction, Embodiment Lab, @ Beijing Institute of Technology \u0026 Beijing Institute for General Artificial Intelligence\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Yixuan Li\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Yixuan Li\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Yixuan Li\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Ph.D. student at the Beijing Institute of Technology.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Yixuan Li's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Yixuan Li\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Ph.D. student at the Beijing Institute of Technology.\"}]],\"error\":null,\"digest\":\"$undefined\"}\n18:{\"metadata\":\"$1e:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>
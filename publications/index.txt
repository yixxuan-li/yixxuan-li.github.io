1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-8723656a45eb45d5.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-8723656a45eb45d5.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-8723656a45eb45d5.js"],"default"]
8:I[9665,[],"MetadataBoundary"]
a:I[9665,[],"OutletBoundary"]
d:I[4911,[],"AsyncMetadataOutlet"]
f:I[9665,[],"ViewportBoundary"]
11:I[6614,[],""]
:HL["/_next/static/css/19bf7477d82f620e.css","style"]
0:{"P":null,"b":"cIHhF2Jmauzj-UtVtoXrW","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/19bf7477d82f620e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"}],"siteTitle":"Yixuan Li","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"Dec 1, 2025"}]]}]}]]}]]}],{"children":[["slug","publications","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",["$","$L8",null,{"children":"$L9"}],null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","HTOq7KjJmh6vdenIdu4hc",{"children":[["$","$Lf",null,{"children":"$L10"}],null]}],null]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
15:I[6669,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","748","static/chunks/748-485beed672c66dd8.js","182","static/chunks/app/%5Bslug%5D/page-1e81779bda6155ca.js"],"default"]
9:["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]
16:T565,Humanoid robot teleoperation plays a vital role in demonstrating and collecting data for complex interactions. Current methods suffer from two key limitations: (1) restricted controllability due to decoupled upper- and lower-body control, and (2) severe drift caused by open-loop execution. These issues prevent humanoid robots from performing coordinated whole-body motions required for long-horizon loco-manipulation tasks. We introduce CLONE, a whole-body teleoperation system that overcomes these challenges through three key contributions: (1) a Mixture-of-Experts (MoE) whole-body control policy that enables complex coordinated movements, such as “picking up an object from the ground” and “placing it in a distant bin”; (2) a closed-loop error correction mechanism using LiDAR odometry, reducing translational drift to 12cm over 8.9-meter trajectories; and (3) a systematic data augmentation strategy that ensures robust performance under diverse, previously unseen operator poses. In extensive experiments, CLONE demonstrates robust performance across diverse scenarios while maintaining stable whole-body control. These capabilities significantly advance humanoid robotics by enabling the collection of long-horizon interaction data and establishing a foundation for more sophisticated humanoid-environment interaction in both research and practical applications.17:T720,@inproceedings{li2025clone,
  title = {CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks},
  author = {Li, Yixuan and Lin, Yutang and Cui, Jieming and Liu, Tengyu and Liang, Wei and Zhu, Yixin and Huang, Siyuan},
  booktitle = {Proceedings of The 9th Conference on Robot Learning (CoRL)},
  year = {2025},
  pages = {https://humanoid-clone.github.io/},
  pdfUrl = {https://arxiv.org/pdf/2506.08931},
  abstract = {Humanoid robot teleoperation plays a vital role in demonstrating and collecting data for complex interactions. Current methods suffer from two key limitations: (1) restricted controllability due to decoupled upper- and lower-body control, and (2) severe drift caused by open-loop execution. These issues prevent humanoid robots from performing coordinated whole-body motions required for long-horizon loco-manipulation tasks. We introduce CLONE, a whole-body teleoperation system that overcomes these challenges through three key contributions: (1) a Mixture-of-Experts (MoE) whole-body control policy that enables complex coordinated movements, such as “picking up an object from the ground” and “placing it in a distant bin”; (2) a closed-loop error correction mechanism using LiDAR odometry, reducing translational drift to 12cm over 8.9-meter trajectories; and (3) a systematic data augmentation strategy that ensures robust performance under diverse, previously unseen operator poses. In extensive experiments, CLONE demonstrates robust performance across diverse scenarios while maintaining stable whole-body control. These capabilities significantly advance humanoid robotics by enabling the collection of long-horizon interaction data and establishing a foundation for more sophisticated humanoid-environment interaction in both research and practical applications.}
}18:T5a9,We propose Reasoning to Ground (R2G), a neural symbolic model that grounds the target objects in 3D scenes in a reasoning manner. Unlike previous works that rely on end-to-end models for grounding, which often function as black boxes, our approach seeks to provide a more interpretable and reliable solution. R2Gexplicitly models the 3D scene using a semantic concept-based scene graph, recurrently simulates the attention transferring across object entities, and interpretably grounding the target objects with the highest attention score. Specifically, we embed multiple object properties within the graph nodes and spatial relations among entities within the edges through a predefined semantic vocabulary. To guide attention transfer, we employ learning or prompting-based approaches to interpret the referential utterance into reasoning instructions within the same semantic space. In each reasoning round, we either (1) merge current attention distribution with the similarity between instructions and embedded entity properties, or (2) shift the attention across the scene graph based on the similarity between instructions and embedded spatial relations. The experiments on Sr3D/Nr3D benchmarks show that R2G achieves a comparable result with the prior works while offering improved interpretability, breaking a new path for 3D grounding. The code and dataset for this work are available at:https://sites.google.com/view/reasoning-to-ground.19:T720,@article{li2025r2g,
  title = {R2G: Reasoning to ground in 3D scenes},
  journal = {Pattern Recognition},
  volume = {168},
  pages = {https://sites.google.com/view/reasoning-to-ground},
  year = {2025},
  doi = {https://doi.org/10.1016/j.patcog.2025.111728},
  author = {Yixuan Li and Zan Wang and Wei Liang},
  pdfUrl = {https://arxiv.org/pdf/2408.13499},
  abstract = {We propose Reasoning to Ground (R2G), a neural symbolic model that grounds the target objects in 3D scenes in a reasoning manner. Unlike previous works that rely on end-to-end models for grounding, which often function as black boxes, our approach seeks to provide a more interpretable and reliable solution. R2Gexplicitly models the 3D scene using a semantic concept-based scene graph, recurrently simulates the attention transferring across object entities, and interpretably grounding the target objects with the highest attention score. Specifically, we embed multiple object properties within the graph nodes and spatial relations among entities within the edges through a predefined semantic vocabulary. To guide attention transfer, we employ learning or prompting-based approaches to interpret the referential utterance into reasoning instructions within the same semantic space. In each reasoning round, we either (1) merge current attention distribution with the similarity between instructions and embedded entity properties, or (2) shift the attention across the scene graph based on the similarity between instructions and embedded spatial relations. The experiments on Sr3D/Nr3D benchmarks show that R2G achieves a comparable result with the prior works while offering improved interpretability, breaking a new path for 3D grounding. The code and dataset for this work are available at:https://sites.google.com/view/reasoning-to-ground.}
}1a:T509,Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce Move to Understand (MTU3D), a unified framework that integrates active perception with 3D vision-language learning, enabling embodied agents to effectively explore and understand their environment. . Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14%, 27%, 11%, and 3% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. MTU3D's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. The deployment on a real robot demonstrates MTU3D's effectiveness in handling real-world data. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.1b:T74c,@inproceedings{zhu2025move,
  author = {Zhu, Ziyu and Wang, Xilin and Li, Yixuan and Zhang, Zhuofan and Ma, Xiaojian and Chen, Yixin and Jia, Baoxiong and Liang, Wei and Yu, Qian and Deng, Zhidong and Huang, Siyuan and Li, Qing},
  title = {Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year = {2025},
  pages = {https://mtu3d.github.io/},
  pdfUrl = {https://arxiv.org/pdf/2507.04047},
  abstract = {Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce Move to Understand (MTU3D), a unified framework that integrates active perception with 3D vision-language learning, enabling embodied agents to effectively explore and understand their environment. . Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14%, 27%, 11%, and 3% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. MTU3D's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. The deployment on a real robot demonstrates MTU3D's effectiveness in handling real-world data. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.}
}1c:T64b,Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.1d:T811,@inproceedings{du2025learning,
  title = {Learning Human-Humanoid Coordination for Collaborative Object Carrying},
  author = {Du, Yushi and Li, Yixuan and Jia, Baoxiong and Lin, Yutang and Zhou, Pei and Liang, Wei and Yang, Yanchao and Huang, Siyuan},
  journal = {arXiv preprint arXiv:2510.14293},
  year = {2025},
  pages = {https://yushi-du.github.io/COLA/},
  pdfUrl = {https://arxiv.org/pdf/2510.14293},
  status = {under-review},
  abstract = {Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.}
}1e:T571,Dogs can climb onto tables using their front legs for support, enabling them to retrieve objects and significantly expand their workspace by leveraging the external environment. However, the ability of quadrupedal robots to perform similar skills remains largely unexplored. In this work, we introduce a unified, learning-based loco-manipulation framework for quadrupedal robots, allowing them to utilize the external environment as support to extend their workspace and enhance their manipulation capabilities. Specifically, our method proposes a unified policy that takes limited onboard sensors and proprioception as input, generating whole-body actions that enable the robot to manipulate objects. To guide the policy learning for environment-in-the-loop manipulation, we design a set of rewards that address challenges such as imprecise perception and center-of-mass shifts. Additionally, we employ curriculum learning to train both teacher and student policies, ensuring effective skill transfer in complex tasks. We train the policy in simulation and conduct extensive experiments, demonstrating that our approach allows robots to manipulate previously inaccessible objects, opening up new possibilities for enhancing quadrupedal robot capabilities without the need for hardware modifications or additional costs. The project page is available at https://sites.google.com/view/env-mani.1f:T737,@inproceedings{li2025env-mani,
  author = {Li, Yixuan and Wang, Zan and Liang, Wei},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Env-Mani: Quadrupedal Robot Loco-Manipulation with Environment-in-the-Loop},
  year = {2025},
  doi = {10.1109/IROS60139.2025.11246108},
  pages = {https://sites.google.com/view/env-mani},
  pdfUrl = {https://ieeexplore.ieee.org/document/11246108},
  abstract = {Dogs can climb onto tables using their front legs for support, enabling them to retrieve objects and significantly expand their workspace by leveraging the external environment. However, the ability of quadrupedal robots to perform similar skills remains largely unexplored. In this work, we introduce a unified, learning-based loco-manipulation framework for quadrupedal robots, allowing them to utilize the external environment as support to extend their workspace and enhance their manipulation capabilities. Specifically, our method proposes a unified policy that takes limited onboard sensors and proprioception as input, generating whole-body actions that enable the robot to manipulate objects. To guide the policy learning for environment-in-the-loop manipulation, we design a set of rewards that address challenges such as imprecise perception and center-of-mass shifts. Additionally, we employ curriculum learning to train both teacher and student policies, ensuring effective skill transfer in complex tasks. We train the policy in simulation and conduct extensive experiments, demonstrating that our approach allows robots to manipulate previously inaccessible objects, opening up new possibilities for enhancing quadrupedal robot capabilities without the need for hardware modifications or additional costs. The project page is available at https://sites.google.com/view/env-mani.}
}20:T4fb,While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.21:T680,@inproceedings{zhu2025human,
  title = {Human Motion Estimation with Everyday Wearables},
  author = {Siqi Zhu and Yixuan Li and Junfu Li and Qi Wu and Zan Wang and Haozhe Ma and Wei Liang},
  year = {2025},
  journal = {arXiv preprint arXiv:2512.21209},
  pages = {https://pie-lab.cn/EveryWear/},
  pdfUrl = {https://arxiv.org/abs/2512.21209},
  status = {under-review},
  abstract = {While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.}
}7:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L15",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"li2025clone","title":"CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks","authors":[{"name":"Yixuan Li","isHighlighted":true,"isCorresponding":false,"isMainAuthor":true},{"name":"Yutang Lin","isHighlighted":false,"isCorresponding":false,"isMainAuthor":true},{"name":"Jieming Cui","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Tengyu Liu","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Wei Liang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false},{"name":"Yixin Zhu","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false},{"name":"Siyuan Huang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of The 9th Conference on Robot Learning (CoRL)","pages":"https://humanoid-clone.github.io/","pdfUrl":"https://arxiv.org/pdf/2506.08931","code":"https://github.com/humanoid-clone/CLONE/","abstract":"$16","description":"CLONE is a whole-body teleoperation system that overcomes the limitations of decoupled upper- and lower-body control and open-loop execution.","selected":true,"preview":"clone.png","bibtex":"$17"},{"id":"li2025r2g","title":"R2G: Reasoning to ground in 3D scenes","authors":[{"name":"Yixuan Li","isHighlighted":true,"isCorresponding":false,"isMainAuthor":false},{"name":"Zan Wang","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Wei Liang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"Pattern Recognition","conference":"","volume":"168","pages":"https://sites.google.com/view/reasoning-to-ground","pdfUrl":"https://arxiv.org/pdf/2408.13499","doi":"https://doi.org/10.1016/j.patcog.2025.111728","code":"https://github.com/yixxuan-li/R2G","abstract":"$18","description":"R2G is a neural symbolic model that grounds the target objects in 3D scenes in a reasoning manner.","selected":true,"preview":"r2g.png","bibtex":"$19"},{"id":"zhu2025move","title":"Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation","authors":[{"name":"Ziyu Zhu","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Xilin Wang","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Yixuan Li","isHighlighted":true,"isCorresponding":false,"isMainAuthor":false},{"name":"Zhuofan Zhang","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Xiaojian Ma","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Yixin Chen","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Baoxiong Jia","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Wei Liang","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Qian Yu","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Zhidong Deng","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false},{"name":"Siyuan Huang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false},{"name":"Qing Li","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)","pages":"https://mtu3d.github.io/","pdfUrl":"https://arxiv.org/pdf/2507.04047","code":"https://github.com/MTU3D/MTU3D","abstract":"$1a","description":"MTU3D is a unified framework that integrates active perception with 3D vision-language learning, enabling embodied agents to effectively explore and understand their environment.","selected":false,"preview":"mtu3d.png","bibtex":"$1b"},{"id":"du2025learning","title":"Learning Human-Humanoid Coordination for Collaborative Object Carrying","authors":[{"name":"Yushi Du","isHighlighted":false,"isCorresponding":false,"isMainAuthor":true},{"name":"Yixuan Li","isHighlighted":true,"isCorresponding":false,"isMainAuthor":true},{"name":"Baoxiong Jia","isHighlighted":false,"isCorresponding":true,"isMainAuthor":true},{"name":"Yutang Lin","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Pei Zhou","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Wei Liang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false},{"name":"Yanchao Yang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false},{"name":"Siyuan Huang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:3:tags","researchArea":"machine-learning","journal":"arXiv preprint arXiv:2510.14293","conference":"","pages":"https://yushi-du.github.io/COLA/","pdfUrl":"https://arxiv.org/pdf/2510.14293","code":"https://github.com/Yushi-Du/COLA_Code","abstract":"$1c","description":"COLA is a proprioception-only reinforcement learning approach that combines leader and follower behaviors within a single policy, enabling compliant human-humanoid collaborative carrying.","selected":true,"preview":"cola.png","bibtex":"$1d"},{"id":"li2025env-mani","title":"Env-Mani: Quadrupedal Robot Loco-Manipulation with Environment-in-the-Loop","authors":[{"name":"Yixuan Li","isHighlighted":true,"isCorresponding":false,"isMainAuthor":false},{"name":"Zan Wang","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Wei Liang","isHighlighted":false,"isCorresponding":true,"isMainAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:4:tags","researchArea":"machine-learning","journal":"","conference":"IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","pages":"https://sites.google.com/view/env-mani","pdfUrl":"https://ieeexplore.ieee.org/document/11246108","doi":"10.1109/IROS60139.2025.11246108","abstract":"$1e","description":"Env-Mani is a unified, learning-based loco-manipulation framework for quadrupedal robots that allows them to utilize the external environment as support to extend their workspace and enhance their manipulation capabilities.","selected":true,"preview":"env_manip.png","bibtex":"$1f"},{"id":"zhu2025human","title":"Human Motion Estimation with Everyday Wearables","authors":[{"name":"Siqi Zhu","isHighlighted":false,"isCorresponding":false,"isMainAuthor":true},{"name":"Yixuan Li","isHighlighted":true,"isCorresponding":false,"isMainAuthor":true},{"name":"Junfu Li","isHighlighted":false,"isCorresponding":false,"isMainAuthor":true},{"name":"Qi Wu","isHighlighted":false,"isCorresponding":false,"isMainAuthor":true},{"name":"Zan Wang","isHighlighted":false,"isCorresponding":false,"isMainAuthor":true},{"name":"Haozhe Ma","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false},{"name":"Wei Liang","isHighlighted":false,"isCorresponding":false,"isMainAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:5:tags","researchArea":"machine-learning","journal":"arXiv preprint arXiv:2512.21209","conference":"","pages":"https://pie-lab.cn/EveryWear/","pdfUrl":"https://arxiv.org/abs/2512.21209","code":"https://github.com/BIT-PIE/EveryWear","abstract":"$20","description":"EveryWear is a lightweight and practical human motion capture approach based entirely on everyday wearables.","selected":false,"preview":"everywear.png","bibtex":"$21"}]}],false,false]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:null
14:{"metadata":[["$","title","0",{"children":"Publications | Yixuan Li"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Yixuan Li"}],["$","meta","3",{"name":"keywords","content":"Yixuan Li,PhD,Research,Perception, Interaction, Embodiment Lab, @ Beijing Institute of Technology & Beijing Institute for General Artificial Intelligence"}],["$","meta","4",{"name":"creator","content":"Yixuan Li"}],["$","meta","5",{"name":"publisher","content":"Yixuan Li"}],["$","meta","6",{"property":"og:title","content":"Yixuan Li"}],["$","meta","7",{"property":"og:description","content":"Ph.D. student at the Beijing Institute of Technology."}],["$","meta","8",{"property":"og:site_name","content":"Yixuan Li's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Yixuan Li"}],["$","meta","13",{"name":"twitter:description","content":"Ph.D. student at the Beijing Institute of Technology."}]],"error":null,"digest":"$undefined"}
e:{"metadata":"$14:metadata","error":null,"digest":"$undefined"}
